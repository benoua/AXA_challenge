{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AXA Report - naBYLa Team\n",
    "\n",
    "<i>Léo Tréguer - leotreguer@hotmail.com, \n",
    "Yann Nicolas - yannnico@gmail.com, Benoit Letournel - benoit.letournel@gmail.com</i>\n",
    "\n",
    "## I / Data pre-processing and first look into the data\n",
    "\n",
    "### 1 - Data importation and column selection\n",
    "\n",
    "We decide to import only those 3 variables into our Jupyter Notebook : \n",
    " - DATE, ASS_ASSIGNMENT as they are the only variables that we could extract from the submission\n",
    " - CSPL_RECEIVED_CALLS as it is exactly the value we are trying to predict\n",
    "\n",
    "This selection also helped us have a slimmer document to work on than the original CSV file. (hopefully, as we would have been limited by our RAM limits) : \n",
    "\n",
    "### 2-  Data cleaning and sum of duplicates \n",
    "\n",
    "We checked that there were no NULL values in the labels. \n",
    "Then, we noticed that there were \"duplicates\" in the data : there were different values for the number of call for the same timeslot and assignment, as shown below. \n",
    "\n",
    "<img src=\"images/duplicates.jpg\">\n",
    "\n",
    "Thus, we did a pandas groupby and Sum for the same DATE and ASS_ASSIGNMENT to concatenate those duplicates.\n",
    "\n",
    "\n",
    "### 3- Data analysis\n",
    "\n",
    "Our first approach was to sum all the calls for all the assignments per day for the whole dataset, in order to have a first picture of the dataset, as shown below. This would also allow us to assess which weeks of data need to be predicted. These \"missing\" weeks are the ones where there is no data on the graph below. \n",
    "\n",
    "<img src=\"images/Total_per_day_2011_2012_2013.jpg\",width=800,height=800>\n",
    "\n",
    "We notice that there is a periodicity in the number of calls, as there seem to be more calls on a Monday than on a Sunday. Moreover, we see that there is a significant increase of the number of calls per day in the second half of 2013, compared to the rest of the dataset, from to the beginning of 2011 to the first half of 2013.\n",
    "\n",
    "Then, we wanted to have a grasp of the number of calls for the 26 different assignments. Note : there are only 24 different assignments for which we have to predict the number of calls in the prediction file, so we will have to drop two useless assignements ('Gestion Amex','Evenements') from the dataset. Below is the sum of calls per month for every assignment.\n",
    "\n",
    "<img src=\"images/Nb of monthly calls per category over 3 years.png\",width=800,height=800>\n",
    "<head>\n",
    "  <title>HTML Reference</title>\n",
    "</head>\n",
    "\n",
    "We notice that there are many different behaviors for the number of calls for every assignment, with little apparent seasonality.\n",
    "The three categories with the largest number of calls are the following : Téléphonie, Tech. Axa and Tech. Inter. The number of calls per day for the assignement \"Téléphonie\" more or less triple compared to the previous period, which explains the increase in the number of total calls highlighted above.\n",
    "The graphs of the number of calls per day for the whole dataset for these three categories are shown below. \n",
    "\n",
    "<img src=\"images/Tech_Inter_2011_2012_2013.jpg\",width=800,height=800>\n",
    "<img src=\"images/Tech_AXA_2011_2012_2013.jpg\",width=800,height=800>\n",
    "<img src=\"images/Telephonie_2011_2012_2013.jpg\",width=800,height=800>\n",
    "\n",
    "As a conclusion, the training set and algorithm used to predict the numbers of calls can be totally different from one category to another : \n",
    "- Tech inter has a pic on August/September each year. We want to predict August 2013 we need to take into account August 2012\n",
    "- Téléphonie has a sharp rise of number of calls after June 2013. Predicting from June depends less on the beginning of the year\n",
    "- Tech. Axa is pretty stable in term of number of calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II / Feature engineering \n",
    "\n",
    "### 1 - Creation of Date features\n",
    "\n",
    "We created new Date features by splitting the Timestamp provided in dataset, as shown below : \n",
    "\n",
    "    X['YEAR'] = X['DATE'].dt.year\n",
    "    X['MONTH'] = X['DATE'].dt.month\n",
    "    X['DAY_nb'] = X['DATE'].dt.day\n",
    "    X['HOUR'] = X['DATE'].dt.hour\n",
    "    X['MIN'] = X['DATE'].dt.minute\n",
    "    X['TIME'] = X['DATE'].dt.time\n",
    "\n",
    "This allowed us to train our model more easily than with the original timestamp format. We also added two new features to the time features :\n",
    "-the day of the week : 0 for Monday, 1 for Tuesday,..\n",
    "-The week of the year : 1 for the first week of the year, 2 for the second week of the year\n",
    "\n",
    "    X['DAY'] = X['DATE'].dt.dayofweek\n",
    "    X['WEEK'] = X['DATE'].dt.week\n",
    "\n",
    "These new features would help our model “catch” the seasonality of call, through a single week (for example, there are usually more calls on a Monday than on a Sunday) and through a year (for example, there are usually fewer calls in summer, during the holidays).\n",
    "    \n",
    "#### Additional date features added : bank holidays: \n",
    "\n",
    "Due to the nature of the business, it seems a fare estimate to add a bank holiday feature in our training set. \n",
    "These days usually represent a decrease in the number of phone calls to the Axa call center. \n",
    "\n",
    "\n",
    "## III / Evaluation - CV pipeline \n",
    "\n",
    "The pipeline that we put in place is the following to cross validate our models is the following.  \n",
    "\n",
    "| Fold      | Training period           |  CV week                    | Submission week\n",
    "|-----------|---------------------------|-----------------------------|---------------------------\n",
    "| 1         | 365 days before CV week   | 7 days from 21 dec 2012     | 7 days from 28 dec 2012\n",
    "| 2         | 365 days before CV week   | 7 days from 26 jan 2013     | 7 days from 2 feb 2013\n",
    "| ...       | ...                       | ...                         | ... \n",
    "\n",
    "Thus, for each week we are required to submit on the leaderboard, we would cv on a week before this submission week and train on one year before the cv week.\n",
    "One week of cross-validation has been chosen due to the periodicity of the pattern on each week. \n",
    "Working with time series, we prefered to choose this pipeline over K-fold cv as we consider that there is a strong correlation between a measure on t and t+1.  \n",
    "\n",
    "\n",
    "## IV / Learning \n",
    "\n",
    "### 1 - Ensemble methods\n",
    "\n",
    "Having selected our features, we wish to start to train a first model on the dataset. \n",
    "\n",
    "As mentioned on the subject, the first direction that we took is to use CART, Random forests and Gradient Boosting Regressors. \n",
    "- from sklearn.tree import DecisionTreeRegressor\n",
    "- from sklearn.ensemble import RandomForestRegressor\n",
    "- from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#### a_ Trees\n",
    "A simple decision tree with depth of 30 gives us a null train error on all 12 training sets but gives us really high values on the CV sets, especially from month 6 (June 2013). \n",
    "At first, we are not surprised by this phenomenom because we are aware that CART does overfit pretty well. \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>week 1</th>\n",
    "      <th>week 2</th>\n",
    "      <th>week 3</th>\n",
    "      <th>week 4</th>\n",
    "      <th>week 5</th>\n",
    "      <th>week 6</th>\n",
    "      <th>week 7</th>\n",
    "      <th>week 8</th>\n",
    "      <th>week 9</th>\n",
    "      <th>week 10</th>\n",
    "      <th>week 11</th>\n",
    "      <th>week 12</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Train</th>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "      <td>0e+00</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>CV</th>\n",
    "      <td>9e+02</td>\n",
    "      <td>3e+02</td>\n",
    "      <td>5e+07</td>\n",
    "      <td>2e+06</td>\n",
    "      <td>1e+06</td>\n",
    "      <td>6e+41</td>\n",
    "      <td>2e+44</td>\n",
    "      <td>4e+42</td>\n",
    "      <td>1e+43</td>\n",
    "      <td>1e+43</td>\n",
    "      <td>8e+48</td>\n",
    "      <td>1e+11</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "#### b_ Random Forests \n",
    "\n",
    "On a second trial, we will examine Random Forests with \n",
    "- n_estimators = 10 (the number of trees in the forest)\n",
    "- criterion : default Mean Squared Errors. Here we can't choose our Linex loss to measure the quality of a split\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>week 1</th>\n",
    "      <th>week 2</th>\n",
    "      <th>week 3</th>\n",
    "      <th>week 4</th>\n",
    "      <th>week 5</th>\n",
    "      <th>week 6</th>\n",
    "      <th>week 7</th>\n",
    "      <th>week 8</th>\n",
    "      <th>week 9</th>\n",
    "      <th>week 10</th>\n",
    "      <th>week 11</th>\n",
    "      <th>week 12</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Train</th>\n",
    "      <td>2e-02</td>\n",
    "      <td>3e-02</td>\n",
    "      <td>3e-02</td>\n",
    "      <td>5e-02</td>\n",
    "      <td>1e-01</td>\n",
    "      <td>9e-01</td>\n",
    "      <td>3e+03</td>\n",
    "      <td>2e-01</td>\n",
    "      <td>1e-01</td>\n",
    "      <td>2e-01</td>\n",
    "      <td>1e+00</td>\n",
    "      <td>4e+01</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>CV</th>\n",
    "      <td>2e+01</td>\n",
    "      <td>2e+03</td>\n",
    "      <td>1e+00</td>\n",
    "      <td>2e+10</td>\n",
    "      <td>5e+04</td>\n",
    "      <td>3e+31</td>\n",
    "      <td>1e+01</td>\n",
    "      <td>7e-01</td>\n",
    "      <td>8e+01</td>\n",
    "      <td>2e+02</td>\n",
    "      <td>6e+12</td>\n",
    "      <td>4e+02</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "The results are more promising as we do not overfit \"as much\" compared to the previous Tree models. RandomForest are usually very good when there is many features. At each nodes, the features are selected randomly to create the split. Knowing scikit learn has an even better handy tool for avoiding overfitting (GradientBoosting), we decide to select this latest.\n",
    "\n",
    "#### c_ Gradient Boosting\n",
    "\n",
    "To set up Gradient Boosting, we use ParameterGrid search isntead of a classic GridSearch. A we are working on the time series, each sample is not iid so we do not want to shuffle it with a k_fold cv. The best parameters found are : \n",
    "{'min_samples_split': 20, 'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 7, 'min_samples_leaf': 30 })\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>week 1</th>\n",
    "      <th>week 2</th>\n",
    "      <th>week 3</th>\n",
    "      <th>week 4</th>\n",
    "      <th>week 5</th>\n",
    "      <th>week 6</th>\n",
    "      <th>week 7</th>\n",
    "      <th>week 8</th>\n",
    "      <th>week 9</th>\n",
    "      <th>week 10</th>\n",
    "      <th>week 11</th>\n",
    "      <th>week 12</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Train</th>\n",
    "      <td>2e-01</td>\n",
    "      <td>4e-01</td>\n",
    "      <td>1e+00</td>\n",
    "      <td>2e+00</td>\n",
    "      <td>2e+01</td>\n",
    "      <td>1e+01</td>\n",
    "      <td>4e+07</td>\n",
    "      <td>1e+08</td>\n",
    "      <td>3e+01</td>\n",
    "      <td>1e+01</td>\n",
    "      <td>3e+01</td>\n",
    "      <td>7e+01</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>CV</th>\n",
    "      <td>1e+01</td>\n",
    "      <td>9e+02</td>\n",
    "      <td>1e+00</td>\n",
    "      <td>1e+06</td>\n",
    "      <td>1e+03</td>\n",
    "      <td>4e+31</td>\n",
    "      <td>7e+03</td>\n",
    "      <td>2e+00</td>\n",
    "      <td>5e+01</td>\n",
    "      <td>6e+01</td>\n",
    "      <td>8e+08</td>\n",
    "      <td>2e+01</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "As expected, we obtain better results with a Gradient Boosting and ParameterSearch. Nevertheless, the errors are still very high especially on June 2013 week. \n",
    "\n",
    "#### d_Errors investigation\n",
    "\n",
    "We ploted errors for Random Forest for the two major categories in terms of errors :  (Téléphonie and Tech AXA)\n",
    "\n",
    "We found out that : \n",
    "\n",
    "a) Errors tends to explode on a specific cross_validation week. This is due to the fact that our linex loss function can \"explode\" pretty fast if the trend of received calls change significantly. We can recall the graph displaying the number of call for Téléphonie over 3 years, we noticed that the number of calls skyrocked at mid-year in 2013. This can account for the major increase we can observe. \n",
    "\n",
    "\n",
    "<img src=\"images/Error_RF.png\",width=400,height=400>\n",
    "\n",
    "b) Secondly, we observe that errors are mainly centered on Mondays, meaning that the model does not make a clear distinction between a week-end and working days. \n",
    "\n",
    "<img src=\"images/Error_RF_DAY.png\",width=400,height=400>\n",
    "\n",
    "\n",
    "#### Conlusion : \n",
    "We had pretty shaky results due to the facts that the models is making mistakes in Mean Square Error but we then assess our errors in Linex. A tiny error for Gradient Boosting can become a significant one for Linex. We want then to come out with a solution that take our loss function directly into account.\n",
    "\n",
    "### 2 - Optimizing the Linex loss function\n",
    "\n",
    "On this section, we are going to try to reformulate the problem with taking into account the Linex loss function. \n",
    "\n",
    "#### a_ Optimization Formulation\n",
    "\n",
    "Supposing that we  want to find a linear solution for our problem. We would have to minimize the function below : \n",
    "\n",
    "$ \\min_{(\\theta)} F(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} linex(y_{i} , \\theta^{\\top}x_{i}) = \\frac{1}{n}\n",
    "\\sum_{i=1}^{n} exp(\\alpha (y_{i} - \\theta^{\\top}x_{i}) ) - \\alpha(y_{i} - \\theta^{\\top}x_{i}) $\n",
    "\n",
    "The derivative of this function is : \n",
    "\n",
    "$ \\frac{\\partial F}{\\partial \\theta} = \\frac{\\alpha}{n}\n",
    "\\sum_{i=1}^{n} [1 - exp(\\alpha (y_{i} - \\theta^{\\top}x_{i})] x_{i}^{\\top}  $\n",
    "\n",
    "Having written those two functions, we will use a L-BFGS descent to find the minimum.\n",
    "\n",
    "For a first trial, we are going to assume that our features are only the Hours&Minutes timestamps. \n",
    "For that we split our training data set per ASS_ASSIGNMENT & DAY. \n",
    "Eg. for a Monday on Téléphonie, the value retained for prediction will depend only on the Hour:Minute.\n",
    "\n",
    "#### b_ Results \n",
    "\n",
    "With the same pipeline defined and a 365 days of training set before each submission week, we obtain an error of 1143. It was so far the best result we had as before it was \"exploding\" due to the nature of the loss function.\n",
    "\n",
    "### 3 - XGBoost \n",
    "\n",
    "To overcome the mistakes made by the MSE on Trees/Random Forests and GradientBoosting we looked at XGBoost which allows the use of custom objectives. \n",
    "\n",
    "Thinking that XGBoost could simplify the problems, we faced several hurdles : \n",
    "- We were not able to install XGBoost on all our computers (only 1 over 3 had a Mac)\n",
    "- We had many difficulties to parameter XGBoost so as to make it work properly\n",
    "- Custom Loss worked only for Classification on our case\n",
    "\n",
    "\n",
    "#### a_ Objectiv formulation for XGBoost\n",
    "\n",
    "To do so we need to compute the gradient and the hessian (actually only the double derivative) of the loss.\n",
    "\n",
    "The linex loss\n",
    "$ F(x) = \\frac{1}{n} \\sum_{i=1}^{n} linex(y_{i} ,x_{i}) = \\frac{1}{n}\n",
    "\\sum_{i=1}^{n} (exp(\\alpha (y_{i} - x_{i}) ) - \\alpha(y_{i} - x_{i}) - 1) $\n",
    "\n",
    "with $ x_{i} $ being the predicted value \n",
    "\n",
    "The gradient of this function is define for each i by : \n",
    "\n",
    "$ \\frac{\\partial F}{\\partial x_{i}} = \\frac{\\alpha}{n}\n",
    "(1 - exp(\\alpha (y_{i} - x_{i}))  $\n",
    "\n",
    "The hessian of this function is define for each i by : \n",
    "\n",
    "$ \\frac{\\partial^{2} F}{\\partial x_{i}^{2}} = \\frac{\\alpha^{2}}{n}\n",
    "exp(\\alpha (y_{i} - x_{i})  $\n",
    "\n",
    "#### b_ XGBoost results\n",
    "\n",
    "One of the issue of using this loss is the risk of overflow, expecially when the labels can differ from 1000. If we have the same initial value, we are bound to have an overflow. Therefore, we need an approximated first value not so far from the result (this is what hinted that we could find simple solutions).\n",
    "\n",
    "Then, we had an implementation issue, using the default loss returned results similar to the gradient boosting above. But using our custom loss had the unfortunate effect to limit drastically the variance in the results, resulting in an overestimation for most of the data (and underestimation for the few high values), except on the month of June where we observed an underestimation for all the data.\n",
    "\n",
    "\n",
    "### 4 - Final solution : Mixed of heuristic approach and Optimization of loss function \n",
    "\n",
    "As we had the best results with the l-bfgs approach so far, and the error only exploded for one result: \"Téléphonie\" and for the month of june, no model based on the past dates manages to predict such a growth in the number of calls.\n",
    "\n",
    "A simple remark is that the growth happens suddenly and is a one time thing. And in our approach we compute our error on this specific week. Then given that the error is the linex we can see that taking the max on the last weeks (same day, same hour, same minute) can outperform most algorithms with the choice of feature we have so far (time feature).\n",
    "\n",
    "#### a_ Feature choice\n",
    "\n",
    "This is why for the \"téléphonie\" categorie we tried a different approach. We replaced the feature (time) by those two value :\n",
    "- the max on the last weeks (same day, same hour, same minute)\n",
    "- the min on the last weeks (same day, same hour, same minute)\n",
    "\n",
    "As these features doesn't contain the days, hours ..., we separated the datasets in 4 : week days vs week-ends, and day vs night.\n",
    "\n",
    "To test this approach we created only small datasets containing values from only a recent past, as the evolution of téléphonie seemed to cause issues, also we removed the week of June that as the one time growth, as it was seen as a cause of overestimation.\n",
    "\n",
    "As we chose the linear regression method (as described in the part2), it seemed best not to add features not related to the values at this point.\n",
    "\n",
    "#### b_ Results\n",
    "\n",
    "\n",
    "This methods gives some results that looks pretty much always as an overestimation based mainly on the max and a little on the min.\n",
    "As the datasets used were relatively small we achieved a convergence after only a few iterations:\n",
    "Here are the evolution of the objective function for the 12 months on the datasets \"week days and day\" that had the maximum values and therefore the biggest potential for underestimation.\n",
    "\n",
    "<img src=\"images/Objectives_Tele.png\">\n",
    "\n",
    "As we can see, we always converge towards a value around 10. But the convergence for June doesn't start really high as there are no values that are above 1000 in the datasets, and this is different for the next months. And even if in June we change the range of values, as we are based on the last weeks values, the errors on the submission set should still be relatively limited.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As a conclusion, we were able to make errors drop significantly during all our progress without really reached the level of the top 10 on the leaderboard. Our difficulties to run XGBoost properly made things impossible for us to reach those levels. \n",
    "\n",
    "Nevertheless, with a more heuristic approach, separating categories and finding a minimizer of our prediction function, wa have succeeded to drop the objectives up to 47. \n",
    "\n",
    "In practice, we think the bump observed on Telephonie is unpredictable. We are not able to predict it unless someone familiar with the AXA phone center advises us about this significant change in pattern. The algorithm choosen should be as much as possible resilient to those kind of events. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
